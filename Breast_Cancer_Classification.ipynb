{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# To make sure we are using gpu not cpu\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "xqKVeK08J3U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-KZ33jO-B-I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten , Dropout , BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator # Changed import statement\n",
        "from keras.callbacks import Callback, ModelCheckpoint, CSVLogger\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx1i4LlsxxRM"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(rescale = 1./255)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # Allows you to upload files from your system\n"
      ],
      "metadata": {
        "id": "AJ1HK563vWbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy the zip file to your Colab environment\n",
        "!cp '/content/drive/My Drive/DataSet.zip' /content/\n",
        "\n",
        "# Unzip the file\n",
        "!unzip /content/DataSet.zip"
      ],
      "metadata": {
        "id": "VyvuDZBrBxLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Unzip the file\n",
        "# with zipfile.ZipFile('DataSet.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('dataset_folder')\n",
        "\n",
        "# # Check the extracted files\n",
        "# os.listdir('dataset_folder')  # This will list the contents of the folder\n"
      ],
      "metadata": {
        "id": "05OzwAvBveed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check the subfolders or files in the extracted dataset\n",
        "# extracted_files = os.listdir('dataset_folder')\n",
        "# print(extracted_files)\n"
      ],
      "metadata": {
        "id": "B7DOB1bqviPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdRUwJ0Zx0Rf"
      },
      "outputs": [],
      "source": [
        "train = datagen.flow_from_directory('train', target_size=(224, 224), class_mode='binary', batch_size=64)\n",
        "# load and iterate validation dataset\n",
        "val = datagen.flow_from_directory('val', target_size=(224, 224), class_mode='binary', batch_size=64)\n",
        "# load and iterate test dataset\n",
        "test = datagen.flow_from_directory('test', target_size=(224, 224), class_mode='binary', batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97frb3aizsGS"
      },
      "outputs": [],
      "source": [
        "imgs, labels = next(train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs.shape"
      ],
      "metadata": {
        "id": "6XFBv599j9Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNK3xw3r0lUP"
      },
      "outputs": [],
      "source": [
        "train.class_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Akvv-n9a0pe_"
      },
      "outputs": [],
      "source": [
        "plt.imshow(imgs[10])\n",
        "print(labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t3oVftO0rAe"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(224,224,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "#https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTzNiVU80vUW"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8S_myBX0yPJ"
      },
      "outputs": [],
      "source": [
        "STEP_SIZE_TRAIN = train.n//train.batch_size\n",
        "STEP_SIZE_VAL = val.n//val.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if the directory exists, if not, create it\n",
        "log_dir = '/content/gdrive/My Drive/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "    print(f\"Created directory: {log_dir}\")\n",
        "\n",
        "# Instantiate the CSVLogger callback\n",
        "csv_logger = CSVLogger(os.path.join(log_dir, 'training.log'), separator=',', append=False)"
      ],
      "metadata": {
        "id": "UT2e6rnngAtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZdTRVx_00SL",
        "outputId": "0fd30e38-fe9f-473e-b8af-f65d3a0a3c7e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 6s/step - accuracy: 0.6208 - loss: 0.6892 - val_accuracy: 0.6692 - val_loss: 0.6450\n",
            "Epoch 2/25\n",
            "\u001b[1m 1/59\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:10\u001b[0m 8s/step - accuracy: 0.7500 - loss: 0.6209"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 458ms/step - accuracy: 0.7500 - loss: 0.6209 - val_accuracy: 0.6923 - val_loss: 0.6172\n",
            "Epoch 3/25\n",
            "\u001b[1m34/59\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 5s/step - accuracy: 0.7021 - loss: 0.6167"
          ]
        }
      ],
      "source": [
        "history = model.fit(x = train,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=test,\n",
        "                    validation_steps=STEP_SIZE_VAL,\n",
        "                    epochs=25,\n",
        "                    callbacks=[csv_logger])\n",
        "model.save('/content/gdrive/My Drive/first_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "log_data = pd.read_csv('/content/gdrive/My Drive/training.log', sep=',', engine='python')"
      ],
      "metadata": {
        "id": "GIPE2DjagI5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "plt.plot(np.arange(1, len(history.history['accuracy'])+1,1), history.history['accuracy'], color='navy', label = 'Accuracy')\n",
        "plt.plot(np.arange(1, len(history.history['accuracy'])+1,1), history.history['val_accuracy'], color='red', label='Validation Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "i6ioGYO037Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the accuracy graph from the saved history (log_data)\n",
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "plt.plot(np.arange(1, len(log_data['accuracy'])+1,1), log_data['accuracy'], color='navy', label = 'Accuracy')\n",
        "plt.plot(np.arange(1, len(log_data['accuracy'])+1,1), log_data['val_accuracy'], color='red', label='Validation Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "53X8reH4-KQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(1, len(history.history['loss'])+1,1), history.history['loss'], color='navy', label = 'Loss')\n",
        "plt.plot(np.arange(1, len(history.history['loss'])+1,1), history.history['val_loss'], color='red', label='Validation Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "jGBLv0aU4VnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the loss graph from the saved history (log_data)\n",
        "\n",
        "plt.plot(np.arange(1, len(log_data['loss'])+1,1), log_data['loss'], color='navy', label = 'Loss')\n",
        "plt.plot(np.arange(1, len(log_data['loss'])+1,1), log_data['val_loss'], color='red', label='Validation Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "CpuY19n6tAmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "restored_model = load_model('/content/gdrive/My Drive/first_model.h5')"
      ],
      "metadata": {
        "id": "FgzkixmIZIc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = test.n//test.batch_size\n",
        "\n",
        "test.reset()\n",
        "X_test, y_test = [] , []\n",
        "for i in range(steps):\n",
        "    a , b = next(test)\n",
        "    X_test.extend(a)\n",
        "    y_test.extend(b)"
      ],
      "metadata": {
        "id": "wDBx4L1huc9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = np.array(X_test), np.array(y_test)"
      ],
      "metadata": {
        "id": "996bKlB5uzKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(X_test, open('/content/gdrive/My Drive/X_test.pkl', 'wb'))\n",
        "pickle.dump(y_test, open('/content/gdrive/My Drive/y_test.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "CtJycGYPHFig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pickle.load(open('/content/gdrive/My Drive/X_test.pkl', 'rb'))\n",
        "y_test = pickle.load(open('/content/gdrive/My Drive/y_test.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "NXnLxwVuHjeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "bILevQN4Y3gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "id": "Xh3PPISd6aDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the saved model\n",
        "score = restored_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "id": "Gx9cZFxdAUwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_prob = model.predict(X_test)"
      ],
      "metadata": {
        "id": "g6nPpeym7B3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the saved model\n",
        "y_pred_prob = restored_model.predict(X_test)"
      ],
      "metadata": {
        "id": "gtROJlK__ZdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_prob"
      ],
      "metadata": {
        "id": "J7bo-3KyKExX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(y_pred_prob,'.',color='red',label='Predicted Probabilty')\n",
        "plt.plot(y_test,'.',color='navy',label='Actual Labels')\n",
        "plt.xlabel('Instance Number')\n",
        "plt.ylabel('Probability')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "b3F6UDuon07E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "HDSmMbdXEQ-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "y_pred = np.where(y_pred_prob > threshold, 1,0)\n",
        "y_pred.squeeze()"
      ],
      "metadata": {
        "id": "pDF_EJD6Cxaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "sns.set(rc={'figure.figsize':(7.7,6.27)})\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test,y_pred),cmap=plt.cm.Blues,annot=True,annot_kws={\"size\": 32}, fmt='g')\n",
        "plt.xticks([0.50,1.50], ['Malignant','Benign'], fontsize=20)\n",
        "plt.yticks([0.50,1.50],['Malignant','Benign'], fontsize=20)\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "plt.title('Confusion Metrix for Breast Cancer')"
      ],
      "metadata": {
        "id": "7mdfDhXnoVtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred, target_names = ['Benign (Class 0)','Malignant (Class 1)']))"
      ],
      "metadata": {
        "id": "6MJaFcNsCNVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr , tpr , thresholds = roc_curve (y_test , y_pred_prob)\n",
        "\n",
        "area_under_curve = auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "9LnEsPd6KMzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(area_under_curve))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9wn49z16KpS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_labels = ['Benign','Malignant']\n",
        "\n",
        "img_indices = np.random.randint(0, len(X_test), size=[25])\n",
        "sample_test_images = X_test[img_indices]\n",
        "sample_test_labels = [cancer_labels[i] for i in y_pred[img_indices].squeeze()]\n",
        "\n",
        "max_prediction = np.argmax(y_pred_prob, axis=1)\n",
        "prediction_probs = np.max(y_pred_prob, axis=1)"
      ],
      "metadata": {
        "id": "Wp0O0fusAUta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "for i, (img, prediction, prob, true_label) in enumerate(\n",
        "    zip(sample_test_images, max_prediction, prediction_probs, sample_test_labels)):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid('off')\n",
        "\n",
        "  plt.imshow(img)\n",
        "  plt.xlabel('{} ({:0.3f})'.format(cancer_labels[prediction], prob))\n",
        "  plt.ylabel('{}'.format(true_label))\n"
      ],
      "metadata": {
        "id": "-BZ8mB6rAUrd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}